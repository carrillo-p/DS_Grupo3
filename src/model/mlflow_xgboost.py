import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, roc_auc_score
import xgboost as xgb
import joblib
import os
import mlflow
import mlflow.sklearn
import time
from threading import Thread
from dotenv import load_dotenv
from datetime import datetime
from BBDD.database import FirebaseInitializer
from azureml.core import Workspace

load_dotenv()

workspace_name = "MLFlow1"
resource_group = "Predictus"
subscription_id = "Azure subscription 1"

ws = Workspace.get(name=workspace_name, resource_group=resource_group, subscription_id=subscription_id)
mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())
mlflow.set_experiment('stroke_prediction_xgboost')

class XGBoostStrokeModel:
    def __init__(self, csv_path=None, model_path=None, scaler_path=None):
        if csv_path:
            self.model = None
            self.scaler = None
            self.feature_names = None
            self.last_check_file = 'last_check.txt'
            self.initial_training_file = 'initial_training_done.txt'
            self.check_interval = int(os.getenv('CHECK_INTERVAL', 3600))
            self.csv_path = csv_path
            
            # Inicializar Firebase
            firebase_init = FirebaseInitializer()
            self.db = firebase_init.db
            self.logger = firebase_init.logger
            self.schema = firebase_init.FIREBASE_SCHEMA['new_data']
            
            self.db_config = {
                'collection': 'new_data',
                'batch_size': int(os.getenv('FIRESTORE_BATCH_SIZE', '1000')),
                'cache_duration': int(os.getenv('FIRESTORE_CACHE_DURATION', '300'))
            }
            self._cached_data = None
            self._last_cache_time = None
        elif model_path and scaler_path:
            self.model = joblib.load('src/model/xgboost_model.joblib')
            self.scaler = joblib.load('src/model/xgb_scaler.joblib')

    def load_data_from_csv(self):
        try:
            df = pd.read_csv(self.csv_path)
            self.feature_names = df.drop('stroke', axis=1).columns
            self.logger.info(f"Datos cargados desde CSV. Número de filas: {len(df)}")
            return df
        except Exception as e:
            self.logger.error(f"Error al cargar datos desde CSV: {e}")
            return pd.DataFrame()

    def _should_refresh_cache(self):
        if self._last_cache_time is None:
            return True
        cache_age = datetime.now() - self._last_cache_time
        return cache_age.total_seconds() > self.db_config['cache_duration']

    def load_data_from_firestore(self):
        try:
            # Check if we can use cached data
            if not self._should_refresh_cache() and self._cached_data is not None:
                self.logger.info("Usando datos en caché")
                return self._cached_data

            self.logger.info("Cargando datos desde Firestore...")
            collection_ref = self.db.collection(self.db_config['collection'])
            
            # Get total document count first (excluding _metadata document)
            query = collection_ref.where('__name__', '!=', '_metadata')
            total_docs = len(list(query.limit(1).get()))
            
            if total_docs == 0:
                return pd.DataFrame()

            # Fetch documents in batches
            all_docs = []
            batch_size = self.db_config['batch_size']
            last_doc = None

            while True:
                if last_doc:
                    query = collection_ref.where('__name__', '!=', '_metadata')\
                                       .limit(batch_size)\
                                       .start_after(last_doc)
                else:
                    query = collection_ref.where('__name__', '!=', '_metadata')\
                                       .limit(batch_size)

                docs = list(query.get())
                if not docs:
                    break

                # Validar cada documento contra el esquema
                for doc in docs:
                    doc_dict = doc.to_dict()
                    if self._validate_document(doc_dict):
                        all_docs.append(doc_dict)
                
                last_doc = docs[-1]
                self.logger.info(f"Cargados {len(all_docs)} documentos de {total_docs}")

            df = pd.DataFrame(all_docs)
            
            if not df.empty:
                self.feature_names = df.drop('stroke', axis=1).columns
                
                # Cache the results
                self._cached_data = df
                self._last_cache_time = datetime.now()
                
                self.logger.info(f"Datos cargados desde Firestore. Número de filas: {len(df)}")
            return df

        except Exception as e:
            self.logger.error(f"Error al cargar datos desde Firestore: {e}")
            return pd.DataFrame()

    def _validate_document(self, doc_dict):
        """Valida un documento contra el esquema definido"""
        required_fields = set(self.schema['fields'].keys())
        doc_fields = set(doc_dict.keys())
        
        # Verificar campos requeridos
        if not required_fields.issubset(doc_fields):
            missing_fields = required_fields - doc_fields
            self.logger.warning(f"Campos faltantes en el documento: {missing_fields}")
            return False
        
        # Validar tipos y valores permitidos
        for field, field_type in self.schema['fields'].items():
            value = doc_dict.get(field)
            if field in self.schema.get('validations', {}):
                if value not in self.schema['validations'][field]:
                    self.logger.warning(f"Valor no válido para {field}: {value}")
                    return False
                    
        return True

    def preprocess_data(self, df):
        if df.empty:
            self.logger.warning("El DataFrame está vacío")
            return None, None
        else:
            if 'stroke' in df.columns:
                X = df.drop('stroke', axis=1)
                y = df['stroke']
                if self.scaler is None:
                    self.scaler = StandardScaler()
                    X_scaled = self.scaler.fit_transform(X)
                else:
                    X_scaled = self.scaler.transform(X)
                return X_scaled, y
            else:
                self.logger.error("La columna 'stroke' no está presente en el DataFrame")
            return None, None

    def train_model(self, X, y):
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        print("Forma de X_train:", X_train.shape)
        print("Forma de y_train:", y_train.shape)
        print("Tipos de datos en X_train:", X_train.dtypes if isinstance(X_train, pd.DataFrame) else X_train.dtype)
        print("Valores únicos en y_train:", np.unique(y_train))
        print("Hay NaN en X_train:", np.isnan(X_train).any())
        print("Hay infinitos en X_train:", np.isinf(X_train).any())
        
        with mlflow.start_run():
            if self.model is None:
                self.model = xgb.XGBClassifier(
                    use_label_encoder=False,
                    eval_metric='logloss',
                    n_estimators=100,
                    max_depth=3,
                    learning_rate=0.1,
                    subsample=0.8,
                    colsample_bytree=0.8,
                    objective='binary:logistic'
                )
            
            try:
                self.model.fit(X_train, y_train)
            except Exception as e:
                print("Error al ajustar el modelo XGBoost:")
                print(str(e))
                raise
            
            mlflow.log_params(self.model.get_params())
            
            y_pred = self.model.predict(X_test)
            accuracy = accuracy_score(y_test, y_pred)
            roc_auc = roc_auc_score(y_test, self.model.predict_proba(X_test)[:, 1])
            
            mlflow.log_metric("accuracy", accuracy)
            mlflow.log_metric("roc_auc", roc_auc)
            
            mlflow.sklearn.log_model(self.model, "model")
            
        return X_test, y_test

    def initial_training(self):
        if not os.path.exists(self.initial_training_file):
            print("Realizando entrenamiento inicial desde CSV...")
            df = self.load_data_from_csv()
            X, y = self.preprocess_data(df)
            self.train_model(X, y)
            with open(self.initial_training_file, 'w') as f:
                f.write('Initial training completed')
            print("Entrenamiento inicial completado.")
        else:
            print("El entrenamiento inicial ya se ha realizado anteriormente.")
            self.load_model('src/model/xgboost_model.joblib', 'src/model/xgb_scaler.joblib')

    def check_and_retrain(self):
        if self.should_check():
            print("Verificando condiciones para reentrenamiento...")
            df = self.load_data_from_firestore()
            
            if len(df) > int(os.getenv('RETRAIN_THRESHOLD', 10000)):
                print("Condiciones cumplidas. Iniciando reentrenamiento...")
                X, y = self.preprocess_data(df)
                self.train_model(X, y)
            else:
                print("No se cumplen las condiciones para reentrenar.")
            
            self.update_last_check()

    def should_check(self):
        if not os.path.exists(self.last_check_file):
            return True
        
        with open(self.last_check_file, 'r') as f:
            last_check = float(f.read().strip())
        
        return time.time() - last_check > self.check_interval

    def update_last_check(self):
        with open(self.last_check_file, 'w') as f:
            f.write(str(time.time()))

    def predict(self, X):
        X_scaled = self.scaler.transform(X)
        return self.model.predict(X_scaled)

    def predict_proba(self, X):
        X_scaled = self.scaler.transform(X)
        return self.model.predict_proba(X_scaled)
    
    def load_model(self, model_path, scaler_path):
        self.model = joblib.load(model_path)
        self.scaler = joblib.load(scaler_path)

def background_worker(model):
    while True:
        model.check_and_retrain()
        time.sleep(model.check_interval)

# Inicialización del modelo y el worker en segundo plano
model = XGBoostStrokeModel(csv_path='src/Data/train_stroke_woe_smote.csv')
model.initial_training()
bg_thread = Thread(target=background_worker, args=(model,))
bg_thread.start()